{"cells":[{"cell_type":"markdown","id":"d97f7c5d-46f3-4cbd-80ad-f1e50cd65096","metadata":{"id":"d97f7c5d-46f3-4cbd-80ad-f1e50cd65096"},"source":["# Deep Learning Applications: Laboratory #1\n","\n","In this first laboratory we will work relatively simple architectures to get a feel for working with Deep Models. This notebook is designed to work with PyTorch, but as I said in the introductory lecture: please feel free to use and experiment with whatever tools you like.\n","\n"]},{"cell_type":"markdown","id":"17ed8906-bd19-4b4f-8b79-4feae355ffd6","metadata":{"id":"17ed8906-bd19-4b4f-8b79-4feae355ffd6"},"source":["## Exercise 1: Warming Up\n","In this series of exercises I want you to try to duplicate (on a small scale) the results of the ResNet paper:\n","\n","> [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385), Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR 2016.\n","\n","We will do this in steps using a Multilayer Perceptron on MNIST.\n","\n","Recall that the main message of the ResNet paper is that **deeper** networks do not **guarantee** more reduction in training loss (or in validation accuracy). Below you will incrementally build a sequence of experiments to verify this for an MLP. A few guidelines:\n","\n","+ I have provided some **starter** code at the beginning. **NONE** of this code should survive in your solutions. Not only is it **very** badly written, it is also written in my functional style that also obfuscates what it's doing (in part to **discourage** your reuse!). It's just to get you *started*.\n","+ These exercises ask you to compare **multiple** training runs, so it is **really** important that you factor this into your **pipeline**. Using [Tensorboard](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html) is a **very** good idea -- or, even better [Weights and Biases](https://wandb.ai/site).\n","+ You may work and submit your solutions in **groups of at most two**. Share your ideas with everyone, but the solutions you submit *must be your own*.\n","\n","First some boilerplate to get you started, then on to the actual exercises!"]},{"cell_type":"markdown","id":"edb2b6d1-3df0-464c-9a5f-8c611257a971","metadata":{"id":"edb2b6d1-3df0-464c-9a5f-8c611257a971"},"source":["### Preface: Some code to get you started\n","\n","What follows is some **very simple** code for training an MLP on MNIST. The point of this code is to get you up and running (and to verify that your Python environment has all needed dependencies).\n","\n","**Note**: As you read through my code and execute it, this would be a good time to think about *abstracting* **your** model definition, and training and evaluation pipelines in order to make it easier to compare performance of different models."]},{"cell_type":"code","execution_count":null,"id":"ab3a8282-2322-4dca-b76e-2f3863bc75fb","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139},"executionInfo":{"elapsed":47694,"status":"ok","timestamp":1715607962046,"user":{"displayName":"Niccolò Arati","userId":"07690613684145163684"},"user_tz":-120},"id":"ab3a8282-2322-4dca-b76e-2f3863bc75fb","outputId":"12301279-9852-4965-f655-ed907b077116","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.3/277.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["# Start with some standard imports.\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from functools import reduce\n","import torch\n","from torchvision.datasets import MNIST\n","from torchvision.datasets import CIFAR10\n","from torch.utils.data import Subset\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","\n","from sklearn.metrics import confusion_matrix\n","\n","from torch.optim.lr_scheduler import MultiStepLR\n","\n","#Tracking experiments\n","%pip install -q wandb\n","import wandb\n","wandb.login()"]},{"cell_type":"markdown","id":"33cc12cc-8422-47bf-8d8e-0950ac05ae96","metadata":{"id":"33cc12cc-8422-47bf-8d8e-0950ac05ae96"},"source":["#### Data preparation\n","\n","Here is some basic dataset loading, validation splitting code to get you started working with MNIST."]},{"cell_type":"code","execution_count":null,"id":"272a69db-0416-444a-9be4-5f055ff48bbb","metadata":{"id":"272a69db-0416-444a-9be4-5f055ff48bbb","tags":[],"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715603402567,"user_tz":-120,"elapsed":3790,"user":{"displayName":"Niccolò Arati","userId":"07690613684145163684"}},"outputId":"bc56a0bd-ff52-4b2b-f5e1-6077a842de57"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 39351002.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 1371461.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 9328818.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 9829994.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n"]}],"source":["# Standard MNIST transform.\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.1307,), (0.3081,))\n","])\n","\n","# Load MNIST train and test.\n","ds_train = MNIST(root='./data', train=True, download=True, transform=transform)\n","ds_test = MNIST(root='./data', train=False, download=True, transform=transform)\n","\n","# Split train into train and validation.\n","val_size = 5000\n","I = np.random.permutation(len(ds_train))\n","ds_val = Subset(ds_train, I[:val_size])\n","ds_train = Subset(ds_train, I[val_size:])"]},{"cell_type":"code","execution_count":null,"id":"427eb937-8beb-44d5-a93d-1d42bee50080","metadata":{"id":"427eb937-8beb-44d5-a93d-1d42bee50080"},"outputs":[],"source":["len(ds_train)"]},{"cell_type":"code","execution_count":null,"id":"70b09e7e-1245-4baa-bcf7-203872d73d3a","metadata":{"id":"70b09e7e-1245-4baa-bcf7-203872d73d3a"},"outputs":[],"source":["len(ds_val), len(ds_test)"]},{"cell_type":"markdown","id":"24e05e96-7707-4490-98b8-50cb5e330af1","metadata":{"id":"24e05e96-7707-4490-98b8-50cb5e330af1"},"source":["#### Boilerplate training and evaluation code\n","\n","This is some **very** rough training, evaluation, and plotting code. Again, just to get you started. I will be *very* disappointed if any of this code makes it into your final submission."]},{"cell_type":"code","execution_count":null,"id":"dbcce348-f603-4d57-b9a8-5b1c6eba28ae","metadata":{"id":"dbcce348-f603-4d57-b9a8-5b1c6eba28ae","tags":[]},"outputs":[],"source":["from tqdm import tqdm #smart progress meter for loops\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Function to train a model for a single epoch over the data loader.\n","def train_epoch(model, dl, opt, epoch='Unknown', device='cpu'):\n","    #Make sure model is in training\n","    model.train() #per salvare i gradienti, sennò non funziona backProp\n","\n","    #capture all batch loss\n","    losses = []\n","\n","    #iterate over all batches in dataloader\n","    for (xs, ys) in tqdm(dl, desc=f'Training epoch {epoch}', leave=True):\n","        xs = xs.to(device) #move images and labels to desired device\n","        ys = ys.to(device)\n","        opt.zero_grad() #zero out gradients\n","        logits = model(xs) #forward pass\n","\n","        #Hardcoded cross-entropy loss\n","        loss = F.cross_entropy(logits, ys)\n","\n","        #compute gradients\n","        loss.backward()\n","\n","        #update parameters\n","        opt.step()\n","\n","        #Save current batch loss\n","        losses.append(loss.item())\n","\n","    #return average loss for this batch\n","    return np.mean(losses)\n","\n","# Function to evaluate model over all samples in the data loader.\n","def evaluate_model(model, dl, device='cpu'):\n","    model.eval()\n","    predictions = []\n","    gts = []\n","    for (xs, ys) in tqdm(dl, desc='Evaluating', leave=False):\n","        xs = xs.to(device)\n","        preds = torch.argmax(model(xs), dim=1)\n","        gts.append(ys)\n","        predictions.append(preds.detach().cpu().numpy())\n","\n","    # Return accuracy score and classification report.\n","    return (accuracy_score(np.hstack(gts), np.hstack(predictions)),\n","            classification_report(np.hstack(gts), np.hstack(predictions), zero_division=0, digits=3))\n","\n","# Simple function to plot the loss curve and validation accuracy.\n","def plot_validation_curves(losses_and_accs):\n","    losses = [x for (x, _) in losses_and_accs]\n","    accs = [x for (_, x) in losses_and_accs]\n","    plt.figure(figsize=(16, 8))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(losses)\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.title('Average Training Loss per Epoch')\n","    plt.subplot(1, 2, 2)\n","    plt.plot(accs)\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Validation Accuracy')\n","    plt.title(f'Best Accuracy = {np.max(accs)} @ epoch {np.argmax(accs)}')"]},{"cell_type":"markdown","id":"875008c3-306c-4e39-a845-d7bda7862621","metadata":{"id":"875008c3-306c-4e39-a845-d7bda7862621"},"source":["#### A basic, parameterized MLP\n","\n","This is a very basic implementation of a Multilayer Perceptron. Don't waste too much time trying to figure out how it works -- the important detail is that it allows you to pass in a list of input, hidden layer, and output *widths*. **Your** implementation should also support this for the exercises to come."]},{"cell_type":"code","execution_count":null,"id":"8c1e503a-37df-4fb9-94e7-85d0adb494bd","metadata":{"id":"8c1e503a-37df-4fb9-94e7-85d0adb494bd","tags":[]},"outputs":[],"source":["#Simple parametrized MLP. Argument layer_sizes provides sizes of all layers\n","#Obfuscated to encourage you to write your own\n","class MLP(nn.Module):\n","    def __init__(self, layer_sizes):\n","        super().__init__()\n","        self.layers = nn.ModuleList([nn.Linear(nin, nout) for (nin, nout) in zip(layer_sizes[:-1], layer_sizes[1:])])\n","\n","    def forward(self, x):\n","        return reduce(lambda f, g: lambda x: g(F.relu(f(x))), self.layers, lambda x: x.flatten(1))(x)"]},{"cell_type":"markdown","id":"4ae06e26-8fa3-414e-a502-8d1c18ba9eb7","metadata":{"id":"4ae06e26-8fa3-414e-a502-8d1c18ba9eb7"},"source":["#### A *very* minimal training pipeline.\n","\n","Here is some basic training and evaluation code to get you started.\n","\n","**Important**: I cannot stress enough that this is a **terrible** example of how to implement a training pipeline. You can do better!"]},{"cell_type":"code","execution_count":null,"id":"fc89e48f-d8f3-4122-842d-1ff389499854","metadata":{"id":"fc89e48f-d8f3-4122-842d-1ff389499854","tags":[],"colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"error","timestamp":1715604284802,"user_tz":-120,"elapsed":274,"user":{"displayName":"Niccolò Arati","userId":"07690613684145163684"}},"outputId":"a79be9c5-3f0b-4e0f-91d0-b40e7c6d6eb3"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'MLP' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-6f975616ecd5>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Instantiate model and optimizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel_mlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdepth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_mlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'MLP' is not defined"]}],"source":["# Training hyperparameters.\n","device = 'cuda' #cuda se su collab\n","epochs = 100\n","lr = 0.0001\n","batch_size = 128\n","\n","# Architecture hyperparameters.\n","input_size = 28*28\n","width = 16\n","depth = 2\n","\n","# Dataloaders.\n","dl_train = torch.utils.data.DataLoader(ds_train, batch_size, shuffle=True, num_workers=4)\n","dl_val   = torch.utils.data.DataLoader(ds_val, batch_size, num_workers=4)\n","dl_test  = torch.utils.data.DataLoader(ds_test, batch_size, shuffle=True, num_workers=4)\n","\n","# Instantiate model and optimizer.\n","model_mlp = MLP([input_size] + [width]*depth + [10]).to(device)\n","opt = torch.optim.Adam(params=model_mlp.parameters(), lr=lr)\n","\n","# Training loop.\n","losses_and_accs = []\n","for epoch in range(epochs):\n","    loss = train_epoch(model_mlp, dl_train, opt, epoch, device=device)\n","    (val_acc, _) = evaluate_model(model_mlp, dl_val, device=device)\n","    losses_and_accs.append((loss, val_acc))\n","\n","# And finally plot the curves.\n","plot_validation_curves(losses_and_accs)\n","print(f'Accuracy report on TEST:\\n {evaluate_model(model_mlp, dl_test, device=device)[1]}')"]},{"cell_type":"markdown","id":"de2cad13-ee2c-4e43-b5c7-31760da8c2df","metadata":{"id":"de2cad13-ee2c-4e43-b5c7-31760da8c2df"},"source":["### Exercise 1.1: A baseline MLP\n","\n","Implement a *simple* Multilayer Perceptron to classify the 10 digits of MNIST (e.g. two *narrow* layers). Use my code above as inspiration, but implement your own training pipeline -- you will need it later. Train this model to convergence, monitoring (at least) the loss and accuracy on the training and validation sets for every epoch. Below I include a basic implementation to get you started -- remember that you should write your *own* pipeline!\n","\n","**Note**: This would be a good time to think about *abstracting* your model definition, and training and evaluation pipelines in order to make it easier to compare performance of different models.\n","\n","**Important**: Given the *many* runs you will need to do, and the need to *compare* performance between them, this would **also** be a great point to study how **Tensorboard** or **Weights and Biases** can be used for performance monitoring."]},{"cell_type":"code","execution_count":null,"id":"19d96405-36e7-4074-803c-fb02576cd528","metadata":{"id":"19d96405-36e7-4074-803c-fb02576cd528","tags":[]},"outputs":[],"source":["class MultiLayerPerceptron(nn.Module):\n","    def __init__(self, input_size = 784, output_size = 10, layers = [32, 32]):\n","        super().__init__()\n","\n","        self.first_layer = nn.Linear(input_size, layers[0])\n","        self.inner_layers = nn.ModuleList([nn.Linear(layers[i], layers[i + 1]) for i in range(len(layers) - 1)])\n","        self.last_layer = nn.Linear(layers[-1], output_size)\n","\n","    def forward(self, x):\n","        x = torch.flatten(x, 1)\n","        x = F.relu(self.first_layer(x))\n","        for layer in self.inner_layers:\n","            x = F.relu(layer(x))\n","        x = self.last_layer(x)\n","        return x\n","\n","#ricordarsi di fare flatten quando si passano le immagini"]},{"cell_type":"code","execution_count":null,"id":"377749f9-ec72-474e-bda7-d00a42db82d1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":272,"status":"ok","timestamp":1715506411441,"user":{"displayName":"Niccolò Arati","userId":"07690613684145163684"},"user_tz":-120},"id":"377749f9-ec72-474e-bda7-d00a42db82d1","outputId":"92c71112-1bbf-448f-ce4a-bca115789a2f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultiLayerPerceptron(\n","  (first_layer): Linear(in_features=784, out_features=32, bias=True)\n","  (inner_layers): ModuleList(\n","    (0-1): 2 x Linear(in_features=32, out_features=32, bias=True)\n","  )\n","  (last_layer): Linear(in_features=32, out_features=10, bias=True)\n",")"]},"metadata":{},"execution_count":4}],"source":["model = MultiLayerPerceptron(layers = [32, 32, 32])\n","model"]},{"cell_type":"code","execution_count":null,"id":"WJF3KAHbJwJM","metadata":{"id":"WJF3KAHbJwJM"},"outputs":[],"source":["from tqdm import tqdm\n","#Trainig loop\n","def train(model, optimizer, dl_train, criterion, epoch, device = 'cuda'):\n","    model.train()\n","    train_losses = []\n","    trn_corr = 0\n","\n","    #for b, (x_train, y_train) in enumerate(dl_train):\n","    for (x_train, y_train) in tqdm(dl_train, desc=f'Training epoch {epoch}', leave=True):\n","        x_train = x_train.to(device)\n","        y_train = y_train.to(device)\n","\n","        optimizer.zero_grad()\n","        y_pred = model(x_train)\n","        loss = criterion(y_pred, y_train)\n","\n","        #predicted = torch.max(y_pred.data, 1)[1] #Prediction with maximum probability\n","        #batch_corr = (predicted == y_train).sum()\n","        #trn_corr += batch_corr\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_losses.append(loss.item())\n","\n","        #if b%300 == 0:\n","            #print(f'epoch: {epoch:2} batch{b:4} [{100*b:6}] Train loss: {loss.item():10.8f} Train accuracy: {batch_corr.item()*100/len(y_train):.3f}%')\n","\n","        return (np.mean(train_losses))"]},{"cell_type":"code","execution_count":null,"id":"DkugHjOlNQJ6","metadata":{"id":"DkugHjOlNQJ6"},"outputs":[],"source":["import pandas as pd\n","import seaborn as sn\n","#Validation\n","#TODO metti loss per vedere overfitting FATTO\n","def evaluate(model, dl_val, val_size, criterion, device = 'cuda', test = False):\n","    model.eval()\n","    val_corr = 0\n","    val_losses = []\n","\n","    # Disable gradient computation and reduce memory consumption.\n","    with torch.no_grad():\n","        for (x_val, y_val) in dl_val:\n","            x_val = x_val.to(device)\n","            y_val = y_val.to(device)\n","\n","            y_pred = model(x_val)\n","            loss_val = criterion(y_pred, y_val)\n","            val_losses.append(loss_val.item())\n","\n","            predicted = torch.max(y_pred.data, 1)[1]\n","            val_corr += (predicted == y_val).sum().item()\n","\n","    val_accuracy = val_corr / val_size\n","    print(f'Validation accuracy: {val_accuracy * 100}%')\n","\n","    if test:\n","        #Build confusion matrix\n","        cf_matrix = confusion_matrix(y_val.detach(), y_pred.detach())\n","        df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],\n","                            columns = [i for i in classes])\n","        plt.figure(figsize = (12,7))\n","        sn.heatmap(df_cm, annot=True)\n","        #plt.savefig('output.png')\n","\n","    return (np.mean(val_losses), val_accuracy)"]},{"cell_type":"code","execution_count":null,"id":"BwwCn0H1Q6u4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3202,"status":"ok","timestamp":1715506441916,"user":{"displayName":"Niccolò Arati","userId":"07690613684145163684"},"user_tz":-120},"id":"BwwCn0H1Q6u4","outputId":"98b57b06-d9f6-48a8-f1d8-25e4350a638a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 34392637.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 1144376.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 10306320.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 9496774.06it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# Standard MNIST transform.\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.1307,), (0.3081,))\n","])\n","\n","# Load MNIST train and test.\n","dataset_train = MNIST(root='./data', train=True, download=True, transform=transform) #60000 images\n","ds_test = MNIST(root='./data', train=False, download=True, transform=transform) #10000 images\n","\n","classes = ('zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine')"]},{"cell_type":"code","execution_count":null,"id":"VcHQZXiOOuSy","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["619c0362f6694bc1b6840091bc57cf13","0c05f5e6b87d434b8bb5732578a0e179","9185de64b3174405883a4df197ec1ef5","b9cee0a6a44d486b8411b74c9442f3ef","2a2be62525cc43e8b325f9c73a3ff7cd","7b556393eaae4da6a8b5bcc0ac7a44cf","317acbbca7f94f0b9d46c9b24e7f9cb7","b2c32e1c27174a2fa62d6967052528bc"]},"executionInfo":{"elapsed":158741,"status":"ok","timestamp":1715506668660,"user":{"displayName":"Niccolò Arati","userId":"07690613684145163684"},"user_tz":-120},"id":"VcHQZXiOOuSy","outputId":"93b5b4b9-6667-4e21-8c50-bda4a674b58d"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mniccolo-arati\u001b[0m (\u001b[33mdla-labs\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.17.0"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20240512_093511-9fhjoh6j</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/dla-labs/Lab1-Final/runs/9fhjoh6j' target=\"_blank\">MLP</a></strong> to <a href='https://wandb.ai/dla-labs/Lab1-Final' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/dla-labs/Lab1-Final' target=\"_blank\">https://wandb.ai/dla-labs/Lab1-Final</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/dla-labs/Lab1-Final/runs/9fhjoh6j' target=\"_blank\">https://wandb.ai/dla-labs/Lab1-Final/runs/9fhjoh6j</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"output_type":"stream","name":"stdout","text":["epoch:  0 batch   0 [     0] Train loss: 2.32400703 Train accuracy: 6.250%\n","epoch:  1 batch   0 [     0] Train loss: 2.29108310 Train accuracy: 10.938%\n","epoch:  2 batch   0 [     0] Train loss: 2.22969604 Train accuracy: 21.094%\n","epoch:  3 batch   0 [     0] Train loss: 2.24307919 Train accuracy: 15.625%\n","epoch:  4 batch   0 [     0] Train loss: 2.21616840 Train accuracy: 18.750%\n","epoch:  5 batch   0 [     0] Train loss: 2.17497015 Train accuracy: 17.969%\n","epoch:  6 batch   0 [     0] Train loss: 2.11090660 Train accuracy: 18.750%\n","epoch:  7 batch   0 [     0] Train loss: 2.06710911 Train accuracy: 28.125%\n","epoch:  8 batch   0 [     0] Train loss: 2.04603338 Train accuracy: 27.344%\n","epoch:  9 batch   0 [     0] Train loss: 2.00872540 Train accuracy: 25.781%\n","epoch: 10 batch   0 [     0] Train loss: 1.93141687 Train accuracy: 32.031%\n","epoch: 11 batch   0 [     0] Train loss: 1.94419205 Train accuracy: 34.375%\n","epoch: 12 batch   0 [     0] Train loss: 1.92872345 Train accuracy: 26.562%\n","epoch: 13 batch   0 [     0] Train loss: 1.88367641 Train accuracy: 35.156%\n","epoch: 14 batch   0 [     0] Train loss: 1.80477595 Train accuracy: 42.969%\n","epoch: 15 batch   0 [     0] Train loss: 1.75073707 Train accuracy: 42.969%\n","epoch: 16 batch   0 [     0] Train loss: 1.73396993 Train accuracy: 35.156%\n","epoch: 17 batch   0 [     0] Train loss: 1.68074465 Train accuracy: 42.969%\n","epoch: 18 batch   0 [     0] Train loss: 1.56262708 Train accuracy: 50.000%\n","epoch: 19 batch   0 [     0] Train loss: 1.58231854 Train accuracy: 41.406%\n","epoch: 20 batch   0 [     0] Train loss: 1.56151938 Train accuracy: 46.094%\n","epoch: 21 batch   0 [     0] Train loss: 1.46731174 Train accuracy: 53.906%\n","epoch: 22 batch   0 [     0] Train loss: 1.51103711 Train accuracy: 51.562%\n","epoch: 23 batch   0 [     0] Train loss: 1.47612822 Train accuracy: 49.219%\n","epoch: 24 batch   0 [     0] Train loss: 1.35389376 Train accuracy: 57.031%\n","epoch: 25 batch   0 [     0] Train loss: 1.38437128 Train accuracy: 61.719%\n","epoch: 26 batch   0 [     0] Train loss: 1.35167408 Train accuracy: 60.938%\n","epoch: 27 batch   0 [     0] Train loss: 1.29122448 Train accuracy: 60.156%\n","epoch: 28 batch   0 [     0] Train loss: 1.31083989 Train accuracy: 68.750%\n","epoch: 29 batch   0 [     0] Train loss: 1.25213790 Train accuracy: 68.750%\n","epoch: 30 batch   0 [     0] Train loss: 1.23101878 Train accuracy: 69.531%\n","epoch: 31 batch   0 [     0] Train loss: 1.07050383 Train accuracy: 80.469%\n","epoch: 32 batch   0 [     0] Train loss: 1.15635717 Train accuracy: 72.656%\n","epoch: 33 batch   0 [     0] Train loss: 1.21935260 Train accuracy: 68.750%\n","epoch: 34 batch   0 [     0] Train loss: 1.02987814 Train accuracy: 76.562%\n","epoch: 35 batch   0 [     0] Train loss: 1.15900838 Train accuracy: 71.094%\n","epoch: 36 batch   0 [     0] Train loss: 1.07296586 Train accuracy: 73.438%\n","epoch: 37 batch   0 [     0] Train loss: 1.04918599 Train accuracy: 73.438%\n","epoch: 38 batch   0 [     0] Train loss: 0.99116826 Train accuracy: 74.219%\n","epoch: 39 batch   0 [     0] Train loss: 0.93998212 Train accuracy: 75.000%\n","epoch: 40 batch   0 [     0] Train loss: 0.93553722 Train accuracy: 78.906%\n","epoch: 41 batch   0 [     0] Train loss: 0.82138252 Train accuracy: 82.031%\n","epoch: 42 batch   0 [     0] Train loss: 0.93711931 Train accuracy: 75.000%\n","epoch: 43 batch   0 [     0] Train loss: 0.86716217 Train accuracy: 73.438%\n","epoch: 44 batch   0 [     0] Train loss: 0.83659184 Train accuracy: 78.125%\n","epoch: 45 batch   0 [     0] Train loss: 0.82010835 Train accuracy: 74.219%\n","epoch: 46 batch   0 [     0] Train loss: 0.79117894 Train accuracy: 81.250%\n","epoch: 47 batch   0 [     0] Train loss: 0.75281012 Train accuracy: 80.469%\n","epoch: 48 batch   0 [     0] Train loss: 0.80516613 Train accuracy: 77.344%\n","epoch: 49 batch   0 [     0] Train loss: 0.82967436 Train accuracy: 78.125%\n","epoch: 50 batch   0 [     0] Train loss: 0.83056307 Train accuracy: 76.562%\n","epoch: 51 batch   0 [     0] Train loss: 0.68244892 Train accuracy: 84.375%\n","epoch: 52 batch   0 [     0] Train loss: 0.69338459 Train accuracy: 81.250%\n","epoch: 53 batch   0 [     0] Train loss: 0.77460611 Train accuracy: 80.469%\n","epoch: 54 batch   0 [     0] Train loss: 0.77910262 Train accuracy: 80.469%\n","epoch: 55 batch   0 [     0] Train loss: 0.73173440 Train accuracy: 80.469%\n","epoch: 56 batch   0 [     0] Train loss: 0.71068674 Train accuracy: 82.812%\n","epoch: 57 batch   0 [     0] Train loss: 0.70973152 Train accuracy: 77.344%\n","epoch: 58 batch   0 [     0] Train loss: 0.63427496 Train accuracy: 86.719%\n","epoch: 59 batch   0 [     0] Train loss: 0.61816466 Train accuracy: 88.281%\n","epoch: 60 batch   0 [     0] Train loss: 0.73645329 Train accuracy: 78.906%\n","epoch: 61 batch   0 [     0] Train loss: 0.61566335 Train accuracy: 83.594%\n","epoch: 62 batch   0 [     0] Train loss: 0.56320041 Train accuracy: 85.156%\n","epoch: 63 batch   0 [     0] Train loss: 0.62829190 Train accuracy: 82.812%\n","epoch: 64 batch   0 [     0] Train loss: 0.59120506 Train accuracy: 85.938%\n","epoch: 65 batch   0 [     0] Train loss: 0.64492929 Train accuracy: 83.594%\n","epoch: 66 batch   0 [     0] Train loss: 0.66813946 Train accuracy: 77.344%\n","epoch: 67 batch   0 [     0] Train loss: 0.56546962 Train accuracy: 85.938%\n","epoch: 68 batch   0 [     0] Train loss: 0.51287621 Train accuracy: 89.844%\n","epoch: 69 batch   0 [     0] Train loss: 0.48163748 Train accuracy: 89.844%\n","epoch: 70 batch   0 [     0] Train loss: 0.56714290 Train accuracy: 85.156%\n","epoch: 71 batch   0 [     0] Train loss: 0.50265908 Train accuracy: 88.281%\n","epoch: 72 batch   0 [     0] Train loss: 0.49440011 Train accuracy: 89.844%\n","epoch: 73 batch   0 [     0] Train loss: 0.74192834 Train accuracy: 77.344%\n","epoch: 74 batch   0 [     0] Train loss: 0.52753943 Train accuracy: 85.156%\n","epoch: 75 batch   0 [     0] Train loss: 0.45760012 Train accuracy: 85.938%\n","epoch: 76 batch   0 [     0] Train loss: 0.59991795 Train accuracy: 85.156%\n","epoch: 77 batch   0 [     0] Train loss: 0.49969369 Train accuracy: 84.375%\n","epoch: 78 batch   0 [     0] Train loss: 0.47065923 Train accuracy: 90.625%\n","epoch: 79 batch   0 [     0] Train loss: 0.47887570 Train accuracy: 87.500%\n","epoch: 80 batch   0 [     0] Train loss: 0.51098490 Train accuracy: 87.500%\n","epoch: 81 batch   0 [     0] Train loss: 0.52922976 Train accuracy: 87.500%\n","epoch: 82 batch   0 [     0] Train loss: 0.55163944 Train accuracy: 83.594%\n","epoch: 83 batch   0 [     0] Train loss: 0.39205050 Train accuracy: 92.188%\n","epoch: 84 batch   0 [     0] Train loss: 0.51482189 Train accuracy: 82.812%\n","epoch: 85 batch   0 [     0] Train loss: 0.55190539 Train accuracy: 85.938%\n","epoch: 86 batch   0 [     0] Train loss: 0.53171694 Train accuracy: 84.375%\n","epoch: 87 batch   0 [     0] Train loss: 0.44851843 Train accuracy: 87.500%\n","epoch: 88 batch   0 [     0] Train loss: 0.43774739 Train accuracy: 84.375%\n","epoch: 89 batch   0 [     0] Train loss: 0.59377062 Train accuracy: 83.594%\n","epoch: 90 batch   0 [     0] Train loss: 0.40084821 Train accuracy: 88.281%\n","epoch: 91 batch   0 [     0] Train loss: 0.46855387 Train accuracy: 89.062%\n","epoch: 92 batch   0 [     0] Train loss: 0.46311557 Train accuracy: 89.062%\n","epoch: 93 batch   0 [     0] Train loss: 0.42571795 Train accuracy: 86.719%\n","epoch: 94 batch   0 [     0] Train loss: 0.47044894 Train accuracy: 85.938%\n","epoch: 95 batch   0 [     0] Train loss: 0.56015515 Train accuracy: 87.500%\n","epoch: 96 batch   0 [     0] Train loss: 0.48435098 Train accuracy: 85.938%\n","epoch: 97 batch   0 [     0] Train loss: 0.43618661 Train accuracy: 88.281%\n","epoch: 98 batch   0 [     0] Train loss: 0.34386975 Train accuracy: 89.062%\n","epoch: 99 batch   0 [     0] Train loss: 0.40349579 Train accuracy: 88.281%\n","Accuracy report on TEST: 88.19999694824219%\n"]},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"619c0362f6694bc1b6840091bc57cf13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁▂▂▃▃▃▄▄▄▅▆▅▆▆▆▇▇▆▇▇▇▇▇█▇▇▇██▇▇███▇▇██▇█</td></tr><tr><td>train_loss</td><td>██▇▇▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▂▃▂▂▂▂▂▂▁▁▂▂▁▁▁▂▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▂▂▂▃▃▄▄▄▅▆▆▆▇▇▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>validation_loss</td><td>████▇▇▆▆▆▆▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>0.882</td></tr><tr><td>train_accuracy</td><td>0.20545</td></tr><tr><td>train_loss</td><td>0.4035</td></tr><tr><td>validation_accuracy</td><td>0.8722</td></tr><tr><td>validation_loss</td><td>0.53141</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">MLP</strong> at: <a href='https://wandb.ai/dla-labs/Lab1-Final/runs/9fhjoh6j' target=\"_blank\">https://wandb.ai/dla-labs/Lab1-Final/runs/9fhjoh6j</a><br/> View project at: <a href='https://wandb.ai/dla-labs/Lab1-Final' target=\"_blank\">https://wandb.ai/dla-labs/Lab1-Final</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20240512_093511-9fhjoh6j/logs</code>"]},"metadata":{}}],"source":["#track run, depth = 3\n","run = wandb.init(\n","    # Set the project where this run will be logged\n","    project=\"Lab1-Final\",\n","    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n","    name=f\"MLP depth 3\",\n","    # Track hyperparameters and run metadata\n","    config={\n","    \"learning_rate\": 1e-3,\n","    \"architecture\": \"MLP\",\n","    \"dataset\": \"mnist\",\n","    \"epochs\": 100,\n","    \"batch_size\": 128,\n","    \"validation_set_size\": 5000,\n","    \"layers\": [32, 32]})\n","\n","# Split train into train and validation.\n","ds_train, ds_val = torch.utils.data.random_split(dataset_train, [60000 - run.config[\"validation_set_size\"], run.config[\"validation_set_size\"]])\n","\n","dl_train = torch.utils.data.DataLoader(ds_train, run.config[\"batch_size\"], shuffle=True, num_workers=2)\n","dl_val   = torch.utils.data.DataLoader(ds_val, run.config[\"batch_size\"], num_workers=2)\n","dl_test  = torch.utils.data.DataLoader(ds_test, run.config[\"batch_size\"], shuffle=True, num_workers=2)\n","\n","#Training and Test\n","#TODO salvare modello con accuracy migliore, usare state_dict() e load_state_dict() FATTO\n","#TODO wandb wandb.Artifact() FATTO\n","device = 'cuda'\n","\n","modelMLP = MultiLayerPerceptron(layers = run.config[\"layers\"]).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(modelMLP.parameters(), lr=run.config[\"learning_rate\"])\n","\n","torch.manual_seed(111)\n","best_accuracy = 0\n","state_dict = None\n","for epoch in range(run.config[\"epochs\"]):\n","    (train_loss, train_corr) = train(modelMLP, optimizer, dl_train, criterion, epoch)\n","    train_accuracy = (train_corr / (60000 - run.config[\"validation_set_size\"])) * 100\n","    metrics = {\"train_loss\": train_loss,\n","               \"train_accuracy\": train_accuracy}\n","\n","    (validation_loss, validation_accuracy) = evaluate(modelMLP, dl_val, run.config[\"validation_set_size\"], criterion)\n","    if validation_accuracy >= best_accuracy:\n","        state_dict = modelMLP.state_dict()\n","    val_metrics = {\"validation_loss\": validation_loss,\n","                   \"validation_accuracy\": validation_accuracy}\n","\n","    wandb.log({**metrics, **val_metrics})\n","\n","#Test\n","modelMLP.load_state_dict(state_dict)\n","\n","(_, test_accuracy) = evaluate(modelMLP, dl_test, 10000, criterion, test=True) #TODO usare modello con migliore validation_accuracy FATTO\n","#TODO usare solo un log, rendere in uscita metriche di interesse FATTO\n","\n","print(f'Accuracy report on TEST: {test_accuracy*100}%')\n","test_metrics = {\"Test Accuracy\": test_accuracy}\n","wandb.log({**test_metrics})\n","\n","wandb.finish()"]},{"cell_type":"markdown","source":["Run aumentando la depth:"],"metadata":{"id":"-wmwzfLmE07X"},"id":"-wmwzfLmE07X"},{"cell_type":"code","source":["#track run, depth = 5\n","run = wandb.init(\n","    # Set the project where this run will be logged\n","    project=\"Lab1-Final\",\n","    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n","    name=f\"MLP depth 5\",\n","    # Track hyperparameters and run metadata\n","    config={\n","    \"learning_rate\": 1e-3,\n","    \"architecture\": \"MLP\",\n","    \"dataset\": \"mnist\",\n","    \"epochs\": 100,\n","    \"batch_size\": 128,\n","    \"validation_set_size\": 5000,\n","    \"layers\": [32, 32, 32, 32]})\n","\n","# Split train into train and validation.\n","ds_train, ds_val = torch.utils.data.random_split(dataset_train, [60000 - run.config[\"validation_set_size\"], run.config[\"validation_set_size\"]])\n","\n","dl_train = torch.utils.data.DataLoader(ds_train, run.config[\"batch_size\"], shuffle=True, num_workers=2)\n","dl_val   = torch.utils.data.DataLoader(ds_val, run.config[\"batch_size\"], num_workers=2)\n","dl_test  = torch.utils.data.DataLoader(ds_test, run.config[\"batch_size\"], shuffle=True, num_workers=2)\n","\n","#Training and Test\n","#TODO salvare modello con accuracy migliore, usare state_dict() e load_state_dict() FATTO\n","#TODO wandb wandb.Artifact() FATTO\n","device = 'cuda'\n","\n","modelMLP = MultiLayerPerceptron(layers = run.config[\"layers\"]).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(modelMLP.parameters(), lr=run.config[\"learning_rate\"])\n","\n","torch.manual_seed(111)\n","best_accuracy = 0\n","state_dict = None\n","for epoch in range(run.config[\"epochs\"]):\n","    (train_loss, train_corr) = train(modelMLP, optimizer, dl_train, criterion, epoch)\n","    train_accuracy = (train_corr / (60000 - run.config[\"validation_set_size\"])) * 100\n","    metrics = {\"train_loss\": train_loss,\n","               \"train_accuracy\": train_accuracy}\n","\n","    (validation_loss, validation_accuracy) = evaluate(modelMLP, dl_val, run.config[\"validation_set_size\"], criterion)\n","    if validation_accuracy >= best_accuracy:\n","        state_dict = modelMLP.state_dict()\n","    val_metrics = {\"validation_loss\": validation_loss,\n","                   \"validation_accuracy\": validation_accuracy}\n","\n","    wandb.log({**metrics, **val_metrics})\n","\n","#Test\n","modelMLP.load_state_dict(state_dict)\n","\n","(_, test_accuracy) = evaluate(modelMLP, dl_test, 10000, criterion, test=True) #TODO usare modello con migliore validation_accuracy FATTO\n","#TODO usare solo un log, rendere in uscita metriche di interesse FATTO\n","\n","print(f'Accuracy report on TEST: {test_accuracy*100}%')\n","test_metrics = {\"Test Accuracy\": test_accuracy}\n","wandb.log({**test_metrics})\n","\n","wandb.finish()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["44b0a6677852405bb53933cc9e3f633e","c7d57407669c420884ad0d712442626e","1c458cbed5d84b42a3700d479f485335","32b64d4a92c74fc6a1397e9b859795a1","8a9f735bc7c6470c9b2c7e6a82a2eea6","0f9c6c3e1eaf4111a79eb20df2fab9c3","d680315b0a7944f9845104aa59bc1a9f","32e4d2ed44fb4f1fa10e52e1019cd2e6"]},"id":"fAcfRpnnE25w","executionInfo":{"status":"ok","timestamp":1715506829101,"user_tz":-120,"elapsed":154521,"user":{"displayName":"Niccolò Arati","userId":"07690613684145163684"}},"outputId":"3cfa992c-2f35-49be-fa4f-4ba6024a4300"},"id":"fAcfRpnnE25w","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.17.0"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20240512_093756-cf01f6jm</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/dla-labs/Lab1/runs/cf01f6jm' target=\"_blank\">MLP</a></strong> to <a href='https://wandb.ai/dla-labs/Lab1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/dla-labs/Lab1' target=\"_blank\">https://wandb.ai/dla-labs/Lab1</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/dla-labs/Lab1/runs/cf01f6jm' target=\"_blank\">https://wandb.ai/dla-labs/Lab1/runs/cf01f6jm</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["epoch:  0 batch   0 [     0] Train loss: 2.30211353 Train accuracy: 12.500%\n","epoch:  1 batch   0 [     0] Train loss: 2.27920604 Train accuracy: 12.500%\n","epoch:  2 batch   0 [     0] Train loss: 2.28554344 Train accuracy: 10.938%\n","epoch:  3 batch   0 [     0] Train loss: 2.30788040 Train accuracy: 15.625%\n","epoch:  4 batch   0 [     0] Train loss: 2.28182125 Train accuracy: 21.094%\n","epoch:  5 batch   0 [     0] Train loss: 2.31561279 Train accuracy: 16.406%\n","epoch:  6 batch   0 [     0] Train loss: 2.28041101 Train accuracy: 20.312%\n","epoch:  7 batch   0 [     0] Train loss: 2.28407502 Train accuracy: 21.094%\n","epoch:  8 batch   0 [     0] Train loss: 2.27117896 Train accuracy: 17.188%\n","epoch:  9 batch   0 [     0] Train loss: 2.27373624 Train accuracy: 17.188%\n","epoch: 10 batch   0 [     0] Train loss: 2.25083494 Train accuracy: 23.438%\n","epoch: 11 batch   0 [     0] Train loss: 2.25625873 Train accuracy: 19.531%\n","epoch: 12 batch   0 [     0] Train loss: 2.24605727 Train accuracy: 20.312%\n","epoch: 13 batch   0 [     0] Train loss: 2.22622991 Train accuracy: 33.594%\n","epoch: 14 batch   0 [     0] Train loss: 2.23290920 Train accuracy: 26.562%\n","epoch: 15 batch   0 [     0] Train loss: 2.24363637 Train accuracy: 18.750%\n","epoch: 16 batch   0 [     0] Train loss: 2.20521522 Train accuracy: 26.562%\n","epoch: 17 batch   0 [     0] Train loss: 2.20101571 Train accuracy: 23.438%\n","epoch: 18 batch   0 [     0] Train loss: 2.17746735 Train accuracy: 28.125%\n","epoch: 19 batch   0 [     0] Train loss: 2.16171408 Train accuracy: 28.906%\n","epoch: 20 batch   0 [     0] Train loss: 2.12228608 Train accuracy: 33.594%\n","epoch: 21 batch   0 [     0] Train loss: 2.10425138 Train accuracy: 39.062%\n","epoch: 22 batch   0 [     0] Train loss: 2.11760950 Train accuracy: 28.125%\n","epoch: 23 batch   0 [     0] Train loss: 2.07183623 Train accuracy: 28.125%\n","epoch: 24 batch   0 [     0] Train loss: 2.04845619 Train accuracy: 33.594%\n","epoch: 25 batch   0 [     0] Train loss: 1.99454713 Train accuracy: 35.156%\n","epoch: 26 batch   0 [     0] Train loss: 1.98655331 Train accuracy: 38.281%\n","epoch: 27 batch   0 [     0] Train loss: 1.89314163 Train accuracy: 41.406%\n","epoch: 28 batch   0 [     0] Train loss: 1.90149283 Train accuracy: 39.062%\n","epoch: 29 batch   0 [     0] Train loss: 1.90275455 Train accuracy: 34.375%\n","epoch: 30 batch   0 [     0] Train loss: 1.88814116 Train accuracy: 28.125%\n","epoch: 31 batch   0 [     0] Train loss: 1.75192058 Train accuracy: 36.719%\n","epoch: 32 batch   0 [     0] Train loss: 1.69016540 Train accuracy: 42.969%\n","epoch: 33 batch   0 [     0] Train loss: 1.73367620 Train accuracy: 42.188%\n","epoch: 34 batch   0 [     0] Train loss: 1.59961855 Train accuracy: 46.875%\n","epoch: 35 batch   0 [     0] Train loss: 1.62813818 Train accuracy: 47.656%\n","epoch: 36 batch   0 [     0] Train loss: 1.57345474 Train accuracy: 53.125%\n","epoch: 37 batch   0 [     0] Train loss: 1.55623472 Train accuracy: 54.688%\n","epoch: 38 batch   0 [     0] Train loss: 1.52318537 Train accuracy: 46.094%\n","epoch: 39 batch   0 [     0] Train loss: 1.43540382 Train accuracy: 48.438%\n","epoch: 40 batch   0 [     0] Train loss: 1.42367804 Train accuracy: 55.469%\n","epoch: 41 batch   0 [     0] Train loss: 1.44036174 Train accuracy: 48.438%\n","epoch: 42 batch   0 [     0] Train loss: 1.29238927 Train accuracy: 59.375%\n","epoch: 43 batch   0 [     0] Train loss: 1.31430864 Train accuracy: 57.031%\n","epoch: 44 batch   0 [     0] Train loss: 1.22812057 Train accuracy: 62.500%\n","epoch: 45 batch   0 [     0] Train loss: 1.18730283 Train accuracy: 60.156%\n","epoch: 46 batch   0 [     0] Train loss: 1.14138734 Train accuracy: 63.281%\n","epoch: 47 batch   0 [     0] Train loss: 1.10040736 Train accuracy: 63.281%\n","epoch: 48 batch   0 [     0] Train loss: 1.17686152 Train accuracy: 55.469%\n","epoch: 49 batch   0 [     0] Train loss: 1.07580006 Train accuracy: 66.406%\n","epoch: 50 batch   0 [     0] Train loss: 1.18376672 Train accuracy: 54.688%\n","epoch: 51 batch   0 [     0] Train loss: 1.06825840 Train accuracy: 64.062%\n","epoch: 52 batch   0 [     0] Train loss: 0.90072143 Train accuracy: 65.625%\n","epoch: 53 batch   0 [     0] Train loss: 0.86089540 Train accuracy: 66.406%\n","epoch: 54 batch   0 [     0] Train loss: 0.97221822 Train accuracy: 62.500%\n","epoch: 55 batch   0 [     0] Train loss: 1.03036821 Train accuracy: 64.062%\n","epoch: 56 batch   0 [     0] Train loss: 1.05082500 Train accuracy: 67.188%\n","epoch: 57 batch   0 [     0] Train loss: 1.09650719 Train accuracy: 60.156%\n","epoch: 58 batch   0 [     0] Train loss: 1.03466034 Train accuracy: 64.844%\n","epoch: 59 batch   0 [     0] Train loss: 0.99715728 Train accuracy: 68.750%\n","epoch: 60 batch   0 [     0] Train loss: 0.92120707 Train accuracy: 64.844%\n","epoch: 61 batch   0 [     0] Train loss: 0.86648524 Train accuracy: 64.844%\n","epoch: 62 batch   0 [     0] Train loss: 0.93658900 Train accuracy: 64.844%\n","epoch: 63 batch   0 [     0] Train loss: 0.97568154 Train accuracy: 64.062%\n","epoch: 64 batch   0 [     0] Train loss: 0.85986662 Train accuracy: 69.531%\n","epoch: 65 batch   0 [     0] Train loss: 0.78069198 Train accuracy: 75.000%\n","epoch: 66 batch   0 [     0] Train loss: 0.76570880 Train accuracy: 77.344%\n","epoch: 67 batch   0 [     0] Train loss: 0.83988190 Train accuracy: 72.656%\n","epoch: 68 batch   0 [     0] Train loss: 0.88889563 Train accuracy: 66.406%\n","epoch: 69 batch   0 [     0] Train loss: 0.85092580 Train accuracy: 67.969%\n","epoch: 70 batch   0 [     0] Train loss: 0.84909093 Train accuracy: 70.312%\n","epoch: 71 batch   0 [     0] Train loss: 0.83586490 Train accuracy: 72.656%\n","epoch: 72 batch   0 [     0] Train loss: 0.79101753 Train accuracy: 70.312%\n","epoch: 73 batch   0 [     0] Train loss: 0.93856877 Train accuracy: 71.094%\n","epoch: 74 batch   0 [     0] Train loss: 0.74443364 Train accuracy: 71.094%\n","epoch: 75 batch   0 [     0] Train loss: 0.82698935 Train accuracy: 75.000%\n","epoch: 76 batch   0 [     0] Train loss: 0.90495652 Train accuracy: 69.531%\n","epoch: 77 batch   0 [     0] Train loss: 0.79296798 Train accuracy: 71.094%\n","epoch: 78 batch   0 [     0] Train loss: 0.60954654 Train accuracy: 79.688%\n","epoch: 79 batch   0 [     0] Train loss: 0.74053037 Train accuracy: 83.594%\n","epoch: 80 batch   0 [     0] Train loss: 0.90280372 Train accuracy: 67.188%\n","epoch: 81 batch   0 [     0] Train loss: 0.89426386 Train accuracy: 70.312%\n","epoch: 82 batch   0 [     0] Train loss: 0.73419279 Train accuracy: 73.438%\n","epoch: 83 batch   0 [     0] Train loss: 0.77566886 Train accuracy: 73.438%\n","epoch: 84 batch   0 [     0] Train loss: 0.69673884 Train accuracy: 74.219%\n","epoch: 85 batch   0 [     0] Train loss: 0.69721305 Train accuracy: 79.688%\n","epoch: 86 batch   0 [     0] Train loss: 0.76244932 Train accuracy: 76.562%\n","epoch: 87 batch   0 [     0] Train loss: 0.70698178 Train accuracy: 80.469%\n","epoch: 88 batch   0 [     0] Train loss: 0.72749436 Train accuracy: 77.344%\n","epoch: 89 batch   0 [     0] Train loss: 0.57541615 Train accuracy: 79.688%\n","epoch: 90 batch   0 [     0] Train loss: 0.76839101 Train accuracy: 68.750%\n","epoch: 91 batch   0 [     0] Train loss: 0.81703597 Train accuracy: 75.781%\n","epoch: 92 batch   0 [     0] Train loss: 0.71179849 Train accuracy: 78.125%\n","epoch: 93 batch   0 [     0] Train loss: 0.70163202 Train accuracy: 79.688%\n","epoch: 94 batch   0 [     0] Train loss: 0.62607056 Train accuracy: 81.250%\n","epoch: 95 batch   0 [     0] Train loss: 0.66521072 Train accuracy: 79.688%\n","epoch: 96 batch   0 [     0] Train loss: 0.70973945 Train accuracy: 82.031%\n","epoch: 97 batch   0 [     0] Train loss: 0.60229963 Train accuracy: 81.250%\n","epoch: 98 batch   0 [     0] Train loss: 0.73347360 Train accuracy: 77.344%\n","epoch: 99 batch   0 [     0] Train loss: 0.61274052 Train accuracy: 82.031%\n","Accuracy report on TEST: 80.75999450683594%\n"]},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.018 MB of 0.018 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44b0a6677852405bb53933cc9e3f633e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁▁▂▂▂▂▂▂▃▃▃▄▃▄▅▄▅▆▆▅▅▆▆▆▆▆█▆▇▇▇█▇▇▇█▇███</td></tr><tr><td>train_loss</td><td>████████▇▇▇▆▆▆▅▅▄▄▃▃▃▂▃▃▂▃▂▂▂▂▂▁▂▂▂▁▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>validation_loss</td><td>████████▇▇▇▇▆▅▄▄▃▃▂▂▂▂▂▁▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>0.8076</td></tr><tr><td>train_accuracy</td><td>0.19091</td></tr><tr><td>train_loss</td><td>0.61274</td></tr><tr><td>validation_accuracy</td><td>0.8046</td></tr><tr><td>validation_loss</td><td>0.83318</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">MLP</strong> at: <a href='https://wandb.ai/dla-labs/Lab1/runs/cf01f6jm' target=\"_blank\">https://wandb.ai/dla-labs/Lab1/runs/cf01f6jm</a><br/> View project at: <a href='https://wandb.ai/dla-labs/Lab1' target=\"_blank\">https://wandb.ai/dla-labs/Lab1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20240512_093756-cf01f6jm/logs</code>"]},"metadata":{}}]},{"cell_type":"code","source":["#track run, depth = 7\n","run = wandb.init(\n","    # Set the project where this run will be logged\n","    project=\"Lab1-Final\",\n","    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n","    name=f\"MLP depth 7\",\n","    # Track hyperparameters and run metadata\n","    config={\n","    \"learning_rate\": 1e-3,\n","    \"architecture\": \"MLP\",\n","    \"dataset\": \"mnist\",\n","    \"epochs\": 100,\n","    \"batch_size\": 128,\n","    \"validation_set_size\": 5000,\n","    \"layers\": [32, 32, 32, 32, 32, 32]})\n","\n","# Split train into train and validation.\n","ds_train, ds_val = torch.utils.data.random_split(dataset_train, [60000 - run.config[\"validation_set_size\"], run.config[\"validation_set_size\"]])\n","\n","dl_train = torch.utils.data.DataLoader(ds_train, run.config[\"batch_size\"], shuffle=True, num_workers=2)\n","dl_val   = torch.utils.data.DataLoader(ds_val, run.config[\"batch_size\"], num_workers=2)\n","dl_test  = torch.utils.data.DataLoader(ds_test, run.config[\"batch_size\"], shuffle=True, num_workers=2)\n","\n","#Training and Test\n","#TODO salvare modello con accuracy migliore, usare state_dict() e load_state_dict() FATTO\n","#TODO wandb wandb.Artifact() FATTO\n","device = 'cuda'\n","\n","modelMLP = MultiLayerPerceptron(layers = run.config[\"layers\"]).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(modelMLP.parameters(), lr=run.config[\"learning_rate\"])\n","\n","torch.manual_seed(111)\n","best_accuracy = 0\n","state_dict = None\n","for epoch in range(run.config[\"epochs\"]):\n","    (train_loss, train_corr) = train(modelMLP, optimizer, dl_train, criterion, epoch)\n","    train_accuracy = (train_corr / (60000 - run.config[\"validation_set_size\"])) * 100\n","    metrics = {\"train_loss\": train_loss,\n","               \"train_accuracy\": train_accuracy}\n","\n","    (validation_loss, validation_accuracy) = evaluate(modelMLP, dl_val, run.config[\"validation_set_size\"], criterion)\n","    if validation_accuracy >= best_accuracy:\n","        state_dict = modelMLP.state_dict()\n","    val_metrics = {\"validation_loss\": validation_loss,\n","                   \"validation_accuracy\": validation_accuracy}\n","\n","    wandb.log({**metrics, **val_metrics})\n","\n","#Test\n","modelMLP.load_state_dict(state_dict)\n","\n","(_, test_accuracy) = evaluate(modelMLP, dl_test, 10000, criterion, test=True) #TODO usare modello con migliore validation_accuracy FATTO\n","#TODO usare solo un log, rendere in uscita metriche di interesse FATTO\n","\n","print(f'Accuracy report on TEST: {test_accuracy*100}%')\n","test_metrics = {\"Test Accuracy\": test_accuracy}\n","wandb.log({**test_metrics})\n","\n","wandb.finish()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["03a4b7ca41e249f2926f497ebdbefd56","329190c8f792403aab0ba443bf2c06ad","01ba87de5f1a4f8a9cf85598ce559f4e","88966ea1556349abbf8170c4f57d0b33","49f856f725b649a38b098407d4c865cd","f3bcdfb4422244489eabb1a6e8f3c6f4","9a3c3fdf4519488ab27e34e77b571b6b","e251ddeffd1b48d88bf4ffea4272bd03"]},"id":"at6cCJYeFBuv","executionInfo":{"status":"ok","timestamp":1715506995235,"user_tz":-120,"elapsed":160781,"user":{"displayName":"Niccolò Arati","userId":"07690613684145163684"}},"outputId":"c18f479d-ba36-475f-fb0c-c4a55834d1c2"},"id":"at6cCJYeFBuv","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.17.0"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20240512_094036-60lquqw7</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/dla-labs/Lab1/runs/60lquqw7' target=\"_blank\">MLP</a></strong> to <a href='https://wandb.ai/dla-labs/Lab1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/dla-labs/Lab1' target=\"_blank\">https://wandb.ai/dla-labs/Lab1</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/dla-labs/Lab1/runs/60lquqw7' target=\"_blank\">https://wandb.ai/dla-labs/Lab1/runs/60lquqw7</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["epoch:  0 batch   0 [     0] Train loss: 2.30879498 Train accuracy: 10.156%\n","epoch:  1 batch   0 [     0] Train loss: 2.30106711 Train accuracy: 11.719%\n","epoch:  2 batch   0 [     0] Train loss: 2.29610491 Train accuracy: 8.594%\n","epoch:  3 batch   0 [     0] Train loss: 2.30321217 Train accuracy: 9.375%\n","epoch:  4 batch   0 [     0] Train loss: 2.29371476 Train accuracy: 10.156%\n","epoch:  5 batch   0 [     0] Train loss: 2.29809833 Train accuracy: 11.719%\n","epoch:  6 batch   0 [     0] Train loss: 2.29303837 Train accuracy: 10.156%\n","epoch:  7 batch   0 [     0] Train loss: 2.30169630 Train accuracy: 10.938%\n","epoch:  8 batch   0 [     0] Train loss: 2.27913928 Train accuracy: 13.281%\n","epoch:  9 batch   0 [     0] Train loss: 2.31024647 Train accuracy: 7.812%\n","epoch: 10 batch   0 [     0] Train loss: 2.29796576 Train accuracy: 10.156%\n","epoch: 11 batch   0 [     0] Train loss: 2.30534649 Train accuracy: 6.250%\n","epoch: 12 batch   0 [     0] Train loss: 2.29549313 Train accuracy: 11.719%\n","epoch: 13 batch   0 [     0] Train loss: 2.27918077 Train accuracy: 14.844%\n","epoch: 14 batch   0 [     0] Train loss: 2.28877759 Train accuracy: 14.062%\n","epoch: 15 batch   0 [     0] Train loss: 2.30825067 Train accuracy: 10.156%\n","epoch: 16 batch   0 [     0] Train loss: 2.28831530 Train accuracy: 14.062%\n","epoch: 17 batch   0 [     0] Train loss: 2.30158877 Train accuracy: 17.969%\n","epoch: 18 batch   0 [     0] Train loss: 2.26935196 Train accuracy: 25.781%\n","epoch: 19 batch   0 [     0] Train loss: 2.27617025 Train accuracy: 26.562%\n","epoch: 20 batch   0 [     0] Train loss: 2.29133344 Train accuracy: 29.688%\n","epoch: 21 batch   0 [     0] Train loss: 2.25437331 Train accuracy: 33.594%\n","epoch: 22 batch   0 [     0] Train loss: 2.25639009 Train accuracy: 37.500%\n","epoch: 23 batch   0 [     0] Train loss: 2.26373076 Train accuracy: 33.594%\n","epoch: 24 batch   0 [     0] Train loss: 2.26597333 Train accuracy: 32.031%\n","epoch: 25 batch   0 [     0] Train loss: 2.25692558 Train accuracy: 38.281%\n","epoch: 26 batch   0 [     0] Train loss: 2.25160217 Train accuracy: 32.812%\n","epoch: 27 batch   0 [     0] Train loss: 2.23579621 Train accuracy: 33.594%\n","epoch: 28 batch   0 [     0] Train loss: 2.22256207 Train accuracy: 33.594%\n","epoch: 29 batch   0 [     0] Train loss: 2.22400856 Train accuracy: 34.375%\n","epoch: 30 batch   0 [     0] Train loss: 2.21680284 Train accuracy: 32.812%\n","epoch: 31 batch   0 [     0] Train loss: 2.20971727 Train accuracy: 28.906%\n","epoch: 32 batch   0 [     0] Train loss: 2.20057726 Train accuracy: 22.656%\n","epoch: 33 batch   0 [     0] Train loss: 2.11051869 Train accuracy: 32.812%\n","epoch: 34 batch   0 [     0] Train loss: 2.14164305 Train accuracy: 26.562%\n","epoch: 35 batch   0 [     0] Train loss: 2.12944078 Train accuracy: 29.688%\n","epoch: 36 batch   0 [     0] Train loss: 2.16026020 Train accuracy: 23.438%\n","epoch: 37 batch   0 [     0] Train loss: 2.14074039 Train accuracy: 24.219%\n","epoch: 38 batch   0 [     0] Train loss: 2.06578732 Train accuracy: 31.250%\n","epoch: 39 batch   0 [     0] Train loss: 2.03007650 Train accuracy: 32.031%\n","epoch: 40 batch   0 [     0] Train loss: 1.97342098 Train accuracy: 37.500%\n","epoch: 41 batch   0 [     0] Train loss: 2.08212399 Train accuracy: 21.094%\n","epoch: 42 batch   0 [     0] Train loss: 1.96548843 Train accuracy: 35.938%\n","epoch: 43 batch   0 [     0] Train loss: 1.96711266 Train accuracy: 39.062%\n","epoch: 44 batch   0 [     0] Train loss: 1.90201855 Train accuracy: 36.719%\n","epoch: 45 batch   0 [     0] Train loss: 1.90614092 Train accuracy: 49.219%\n","epoch: 46 batch   0 [     0] Train loss: 1.80535972 Train accuracy: 53.125%\n","epoch: 47 batch   0 [     0] Train loss: 1.76695669 Train accuracy: 57.031%\n","epoch: 48 batch   0 [     0] Train loss: 1.78276432 Train accuracy: 56.250%\n","epoch: 49 batch   0 [     0] Train loss: 1.70395970 Train accuracy: 49.219%\n","epoch: 50 batch   0 [     0] Train loss: 1.75130415 Train accuracy: 49.219%\n","epoch: 51 batch   0 [     0] Train loss: 1.61583471 Train accuracy: 50.781%\n","epoch: 52 batch   0 [     0] Train loss: 1.50856352 Train accuracy: 54.688%\n","epoch: 53 batch   0 [     0] Train loss: 1.45942330 Train accuracy: 57.031%\n","epoch: 54 batch   0 [     0] Train loss: 1.54890311 Train accuracy: 52.344%\n","epoch: 55 batch   0 [     0] Train loss: 1.44882929 Train accuracy: 54.688%\n","epoch: 56 batch   0 [     0] Train loss: 1.47964394 Train accuracy: 53.906%\n","epoch: 57 batch   0 [     0] Train loss: 1.34753990 Train accuracy: 54.688%\n","epoch: 58 batch   0 [     0] Train loss: 1.39602447 Train accuracy: 57.812%\n","epoch: 59 batch   0 [     0] Train loss: 1.22618008 Train accuracy: 58.594%\n","epoch: 60 batch   0 [     0] Train loss: 1.24994564 Train accuracy: 57.031%\n","epoch: 61 batch   0 [     0] Train loss: 1.25244391 Train accuracy: 58.594%\n","epoch: 62 batch   0 [     0] Train loss: 1.31239307 Train accuracy: 58.594%\n","epoch: 63 batch   0 [     0] Train loss: 1.16934276 Train accuracy: 61.719%\n","epoch: 64 batch   0 [     0] Train loss: 1.18314540 Train accuracy: 58.594%\n","epoch: 65 batch   0 [     0] Train loss: 1.05440092 Train accuracy: 66.406%\n","epoch: 66 batch   0 [     0] Train loss: 1.17799437 Train accuracy: 67.969%\n","epoch: 67 batch   0 [     0] Train loss: 1.10802209 Train accuracy: 66.406%\n","epoch: 68 batch   0 [     0] Train loss: 1.05011976 Train accuracy: 69.531%\n","epoch: 69 batch   0 [     0] Train loss: 0.96726674 Train accuracy: 67.188%\n","epoch: 70 batch   0 [     0] Train loss: 1.16671920 Train accuracy: 64.062%\n","epoch: 71 batch   0 [     0] Train loss: 1.19552636 Train accuracy: 64.844%\n","epoch: 72 batch   0 [     0] Train loss: 1.04947650 Train accuracy: 60.156%\n","epoch: 73 batch   0 [     0] Train loss: 1.08669782 Train accuracy: 66.406%\n","epoch: 74 batch   0 [     0] Train loss: 0.96842837 Train accuracy: 65.625%\n","epoch: 75 batch   0 [     0] Train loss: 1.09337831 Train accuracy: 57.812%\n","epoch: 76 batch   0 [     0] Train loss: 1.16863155 Train accuracy: 64.062%\n","epoch: 77 batch   0 [     0] Train loss: 1.00836611 Train accuracy: 63.281%\n","epoch: 78 batch   0 [     0] Train loss: 0.92619985 Train accuracy: 67.969%\n","epoch: 79 batch   0 [     0] Train loss: 0.95649695 Train accuracy: 69.531%\n","epoch: 80 batch   0 [     0] Train loss: 1.10589135 Train accuracy: 59.375%\n","epoch: 81 batch   0 [     0] Train loss: 1.07176292 Train accuracy: 71.875%\n","epoch: 82 batch   0 [     0] Train loss: 0.90026265 Train accuracy: 67.969%\n","epoch: 83 batch   0 [     0] Train loss: 1.00879550 Train accuracy: 65.625%\n","epoch: 84 batch   0 [     0] Train loss: 1.00911558 Train accuracy: 68.750%\n","epoch: 85 batch   0 [     0] Train loss: 0.86050916 Train accuracy: 71.875%\n","epoch: 86 batch   0 [     0] Train loss: 1.03850245 Train accuracy: 70.312%\n","epoch: 87 batch   0 [     0] Train loss: 0.81798375 Train accuracy: 75.781%\n","epoch: 88 batch   0 [     0] Train loss: 0.96771061 Train accuracy: 66.406%\n","epoch: 89 batch   0 [     0] Train loss: 0.84375370 Train accuracy: 72.656%\n","epoch: 90 batch   0 [     0] Train loss: 1.00972378 Train accuracy: 67.188%\n","epoch: 91 batch   0 [     0] Train loss: 0.97367316 Train accuracy: 71.875%\n","epoch: 92 batch   0 [     0] Train loss: 0.96118605 Train accuracy: 70.312%\n","epoch: 93 batch   0 [     0] Train loss: 0.91083139 Train accuracy: 73.438%\n","epoch: 94 batch   0 [     0] Train loss: 0.96222621 Train accuracy: 67.188%\n","epoch: 95 batch   0 [     0] Train loss: 0.77566832 Train accuracy: 78.125%\n","epoch: 96 batch   0 [     0] Train loss: 0.87518710 Train accuracy: 70.312%\n","epoch: 97 batch   0 [     0] Train loss: 0.81425118 Train accuracy: 72.656%\n","epoch: 98 batch   0 [     0] Train loss: 0.82173872 Train accuracy: 74.219%\n","epoch: 99 batch   0 [     0] Train loss: 0.91259879 Train accuracy: 71.094%\n","Accuracy report on TEST: 71.01000213623047%\n"]},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.018 MB of 0.018 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03a4b7ca41e249f2926f497ebdbefd56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▂▃▄▄▄▄▄▃▃▄▄▅▆▅▆▆▆▆▇▇█▇▇▇▇█▇█▇████</td></tr><tr><td>train_loss</td><td>█████████████▇▇▇▆▆▆▅▅▄▄▄▃▂▂▂▃▂▂▁▂▂▂▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▂▂▃▄▄▄▄▃▃▃▃▃▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇████████</td></tr><tr><td>validation_loss</td><td>██████████████▇▇▇▇▆▆▆▅▄▄▃▂▂▂▁▂▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>0.7101</td></tr><tr><td>train_accuracy</td><td>0.16545</td></tr><tr><td>train_loss</td><td>0.9126</td></tr><tr><td>validation_accuracy</td><td>0.7166</td></tr><tr><td>validation_loss</td><td>0.61752</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">MLP</strong> at: <a href='https://wandb.ai/dla-labs/Lab1/runs/60lquqw7' target=\"_blank\">https://wandb.ai/dla-labs/Lab1/runs/60lquqw7</a><br/> View project at: <a href='https://wandb.ai/dla-labs/Lab1' target=\"_blank\">https://wandb.ai/dla-labs/Lab1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20240512_094036-60lquqw7/logs</code>"]},"metadata":{}}]},{"cell_type":"markdown","id":"0fb8ad9b-e3ae-4c49-9bec-35aaea149b08","metadata":{"id":"0fb8ad9b-e3ae-4c49-9bec-35aaea149b08"},"source":["### Exercise 1.2: Rinse and Repeat\n","\n","Repeat the verification you did above, but with **Convolutional** Neural Networks. If you were careful about abstracting your model and training code, this should be a simple exercise. Show that **deeper** CNNs *without* residual connections do not always work better and **even deeper** ones *with* residual connections.\n","\n","**Hint**: You probably should do this exercise using CIFAR10, since MNIST is *very* easy (at least up to about 99% accuracy).\n","\n","**Spoiler**: If you plan to do optional exercise 2.3, you should think *very* carefully about the architectures of your CNNs here (so you can reuse them!)."]},{"cell_type":"code","execution_count":null,"id":"LUlt2x637zDV","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13006,"status":"ok","timestamp":1715607993810,"user":{"displayName":"Niccolò Arati","userId":"07690613684145163684"},"user_tz":-120},"id":"LUlt2x637zDV","outputId":"786493d7-dcd9-472f-c887-a6ca3c9d1111"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:02<00:00, 68827425.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"]}],"source":["#load CIFAR-10\n","train_transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.RandomHorizontalFlip(p=0.5),\n","     transforms.RandomAffine(degrees = 10, scale = (0.8, 1.2)),\n","     transforms.ColorJitter(brightness = 0.2, contrast = 0.2, saturation = 0.2),\n","     transforms.RandomCrop(28),\n","     transforms.Resize((32, 32)),\n","     transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2470, 0.2435, 0.2616])])\n","\n","test_transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n","\n","trainset = CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n","ds_test = CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","#TODO pensare se aggiungere trasformazioni, in caso controllare che testing abbia solo normalize FATTO\n","#TODO mettere confusion matrix? FATTO\n","#TODO guardare resnet FATTO\n","#TODO pensare se usare learning_rate scheduler FATTO"]},{"cell_type":"code","execution_count":null,"id":"Yom6gIceuGVS","metadata":{"id":"Yom6gIceuGVS"},"outputs":[],"source":["class BasicBlock(nn.Module):\n","    def __init__(self, inplanes, planes, size = 3, stride = 1, downsample = None, padding = 1, residual=False):\n","        super().__init__()\n","        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=size, stride=stride, padding=padding, bias=False)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=size, padding=padding, bias=False)\n","        self.residual = residual\n","        self.downsample = downsample\n","\n","    def forward(self, x):\n","        identity = x.clone()\n","\n","        x = self.conv1(x)\n","        x = self.relu(x)\n","\n","        x = self.conv2(x)\n","\n","        if self.residual:\n","            if self.downsample is not None:\n","                identity = self.downsample(identity)\n","            x += identity\n","        x = self.relu(x)\n","\n","        return x\n","\n","#layers: numero di blocchi per ogni layer\n","class CNN(nn.Module):\n","    def __init__(self, layers, num_classes = 10, residual=False):\n","        super().__init__()\n","\n","        self.residual = residual\n","        self.inplanes = 64\n","        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True)\n","        self.layer1 = self.make_layer(64, layers[0])\n","        self.layer2 = self.make_layer(128, layers[1], stride=2)\n","        self.layer3 = self.make_layer(256, layers[2], stride=2)\n","        #self.layer4 = self.make_layer(512, layers[3], stride=2)\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(256, num_classes)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                #nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n","                m.weight.data.normal_(0, 0.01)\n","\n","    def make_layer(self, planes, blocks, stride = 1):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes:\n","            downsample = nn.Conv2d(self.inplanes, planes, kernel_size = 1, stride = stride, bias = False)\n","\n","        layers = []\n","        layers.append(BasicBlock(self.inplanes, planes, stride=stride, downsample=downsample, residual=self.residual))\n","        self.inplanes = planes\n","        for _ in range(1, blocks):\n","            layers.append(BasicBlock(self.inplanes, planes, residual=self.residual))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        #x = self.layer4(x)\n","\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc(x)\n","\n","        return x"]},{"cell_type":"code","source":["#Check model\n","x = torch.randn(size=(1, 3, 28, 28))\n","x = x.to('cuda')\n","model_ex = CNN(layers = [1, 3, 2]).to('cuda')\n","\n","print(model_ex)\n","\n","print(model_ex.conv1(x).size())\n","print(model_ex.maxpool(model_ex.conv1(x)).size())\n","print(model_ex.layer1(model_ex.maxpool(model_ex.conv1(x))).size())\n","print(model_ex.layer2(model_ex.layer1(model_ex.maxpool(model_ex.conv1(x)))).size())\n","print(model_ex.layer3(model_ex.layer2(model_ex.layer1(model_ex.maxpool(model_ex.conv1(x))))).size())\n","print(model_ex.avgpool(model_ex.layer3(model_ex.layer2(model_ex.layer1(model_ex.maxpool(model_ex.conv1(x)))))).size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XM-S_Xl5Ywxh","executionInfo":{"status":"ok","timestamp":1715606888568,"user_tz":-120,"elapsed":1735,"user":{"displayName":"Niccolò Arati","userId":"07690613684145163684"}},"outputId":"1ce3f475-fcfc-4d1a-aa5f-fa6daaf8890a"},"id":"XM-S_Xl5Ywxh","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CNN(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=True)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (downsample): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    )\n","    (2): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (downsample): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=256, out_features=10, bias=True)\n",")\n","torch.Size([1, 64, 14, 14])\n","torch.Size([1, 64, 8, 8])\n","torch.Size([1, 64, 8, 8])\n","torch.Size([1, 128, 4, 4])\n","torch.Size([1, 256, 2, 2])\n","torch.Size([1, 256, 1, 1])\n"]}]},{"cell_type":"markdown","id":"06e10d13","metadata":{"id":"06e10d13"},"source":["Runs with CNN without residual connection"]},{"cell_type":"code","execution_count":null,"id":"gKlBHJoiROr2","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"executionInfo":{"elapsed":2576,"status":"error","timestamp":1715608025800,"user":{"displayName":"Niccolò Arati","userId":"07690613684145163684"},"user_tz":-120},"id":"gKlBHJoiROr2","outputId":"e070b282-3512-4419-cb58-382405b7c9fb"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Finishing last run (ID:8np3xuz6) before initializing another..."]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Successfully finished last run (ID:8np3xuz6). Initializing new run:<br/>"]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-29a50a82fd37>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#track run, depth = 8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m run = wandb.init(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Set the project where this run will be logged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Lab1-Final\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, settings)\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0mwi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_WandbInit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0mrun_init_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeliver_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m         result = run_init_handle.wait(\n\u001b[0m\u001b[1;32m    748\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mon_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_progress_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/mailbox.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout, on_probe, on_progress, release, cancel)\u001b[0m\n\u001b[1;32m    281\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mMailboxError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transport failed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0mfound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabandoned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_and_clear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;31m# Always update progress to 100% when done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/mailbox.py\u001b[0m in \u001b[0;36m_get_and_clear\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_and_clear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/mailbox.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_and_clear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["torch.manual_seed(12)\n","#track run, depth = 8\n","run = wandb.init(\n","    # Set the project where this run will be logged\n","    project=\"Lab1-Final\",\n","    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n","    name=f\"CNN depth 8\",\n","    # Track hyperparameters and run metadata\n","    config={\n","    \"initial_learning_rate\": 0.001,\n","    \"architecture\": \"CNN\",\n","    \"dataset\": \"Cifar10\",\n","    \"epochs\": 200,\n","    \"batch_size\": 256,\n","    \"validation_set_size\": 5000,\n","    \"layers\": [1, 1, 1]}) #TODO aggiungere depth FATTO\n","\n","#Split into train and validation\n","np.random.seed(111)\n","#ds_train, ds_val = torch.utils.data.random_split(trainset, [50000 - run.config[\"validation_set_size\"], run.config[\"validation_set_size\"]])\n","I = np.random.permutation(len(trainset))\n","ds_val = Subset(trainset, I[:run.config[\"validation_set_size\"]])\n","ds_train = Subset(trainset, I[run.config[\"validation_set_size\"]:])\n","\n","# Dataloaders\n","dl_train = torch.utils.data.DataLoader(ds_train, batch_size=run.config[\"batch_size\"], shuffle=True)\n","dl_val   = torch.utils.data.DataLoader(ds_val, batch_size=run.config[\"batch_size\"])\n","dl_test = torch.utils.data.DataLoader(ds_test, batch_size=run.config[\"batch_size\"], shuffle=False)\n","\n","#Training and Test\n","device = 'cuda'\n","\n","modelCNN = CNN(layers=run.config[\"layers\"]).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(modelCNN.parameters(), lr=run.config[\"initial_learning_rate\"])\n","scheduler = MultiStepLR(optimizer, milestones=[80, 160], gamma=0.1)\n","\n","best_accuracy = 0\n","state_dict = None\n","for epoch in range(run.config[\"epochs\"]):\n","    (train_loss) = train(modelCNN, optimizer, dl_train, criterion, epoch)\n","    #train_accuracy = (train_corr / (50000 - run.config[\"validation_set_size\"])) * 100\n","    metrics = {\"train_loss\": train_loss}\n","\n","    (validation_loss, validation_accuracy) = evaluate(modelCNN, dl_val, run.config[\"validation_set_size\"], criterion)\n","    if validation_accuracy >= best_accuracy:\n","        state_dict = modelCNN.state_dict()\n","    val_metrics = {\"validation_loss\": validation_loss,\n","                   \"validation_accuracy\": validation_accuracy}\n","\n","    scheduler.step()\n","\n","    wandb.log({**metrics, **val_metrics})\n","\n","#Test\n","modelCNN.load_state_dict(state_dict)\n","\n","#save model on weights and biases\n","model_artifact = wandb.Artifact(\n","    \"CNN\", type=\"model\",\n","    description=\"best model\",\n","    metadata=dict(run.config))\n","\n","torch.save(modelCNN.state_dict(), \"best_model_cnn.pth\")\n","model_artifact.add_file(\"best_model_cnn.pth\")\n","wandb.save(\"best_model_cnn.pth\")\n","run.log_artifact(model_artifact)\n","\n","(_, test_accuracy) = evaluate(modelCNN, dl_test, 10000, criterion, test=True) #TODO usare modello con migliore validation_accuracy FATTO\n","#TODO usare solo un log, rendere in uscita metriche di interesse FATTO\n","\n","print(f'Accuracy report on TEST: {test_accuracy*100}%')\n","test_metrics = {\"Test Accuracy\": test_accuracy}\n","wandb.log({**test_metrics})\n","\n","wandb.finish()"]},{"cell_type":"code","source":["#track run, depth = 10\n","run = wandb.init(\n","    # Set the project where this run will be logged\n","    project=\"Lab1-Final\",\n","    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n","    name=f\"CNN depth 10\",\n","    # Track hyperparameters and run metadata\n","    config={\n","    \"initial_learning_rate\": 0.1,\n","    \"architecture\": \"CNN\",\n","    \"dataset\": \"Cifar10\",\n","    \"epochs\": 200,\n","    \"batch_size\": 128,\n","    \"validation_set_size\": 5000,\n","    \"layers\": [1, 2, 1]}) #TODO aggiungere depth FATTO\n","\n","#Split into train and validation\n","ds_train, ds_val = torch.utils.data.random_split(trainset, [50000 - run.config[\"validation_set_size\"], run.config[\"validation_set_size\"]])\n","\n","# Dataloaders\n","dl_train = torch.utils.data.DataLoader(ds_train, batch_size=run.config[\"batch_size\"], shuffle=True, num_workers=2)\n","dl_val   = torch.utils.data.DataLoader(ds_val, batch_size=run.config[\"batch_size\"], num_workers=2)\n","dl_test = torch.utils.data.DataLoader(ds_test, batch_size=run.config[\"batch_size\"], shuffle=False, num_workers=2)\n","\n","#Training and Test\n","device = 'cuda'\n","\n","modelCNN = CNN(layers=[1,1,1]).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(modelCNN.parameters(), lr=run.config[\"initial_learning_rate\"])\n","scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1, end_factor=0.2, total_iters=100)\n","\n","torch.manual_seed(111)\n","best_accuracy = 0\n","state_dict = None\n","for epoch in range(run.config[\"epochs\"]):\n","    (train_loss, train_corr) = train(modelCNN, optimizer, dl_train, criterion, epoch)\n","    train_accuracy = (train_corr / (50000 - run.config[\"validation_set_size\"])) * 100\n","    metrics = {\"train_loss\": train_loss,\n","               \"train_accuracy\": train_accuracy}\n","\n","    (validation_loss, validation_accuracy) = evaluate(modelCNN, dl_val, run.config[\"validation_set_size\"], criterion)\n","    if validation_accuracy >= best_accuracy:\n","        state_dict = modelCNN.state_dict()\n","    val_metrics = {\"validation_loss\": validation_loss,\n","                   \"validation_accuracy\": validation_accuracy}\n","\n","    scheduler.step()\n","\n","    wandb.log({**metrics, **val_metrics})\n","\n","#Test\n","modelCNN.load_state_dict(state_dict)\n","\n","#save model on weights and biases\n","model_artifact = wandb.Artifact(\n","    \"CNN\", type=\"model\",\n","    description=\"best model\",\n","    metadata=dict(run.config))\n","\n","torch.save(modelCNN.state_dict(), \"best_model_cnn.pth\")\n","model_artifact.add_file(\"best_model_cnn.pth\")\n","wandb.save(\"best_model_cnn.pth\")\n","run.log_artifact(model_artifact)\n","\n","(_, test_accuracy) = evaluate(modelCNN, dl_test, 10000, criterion, test=True) #TODO usare modello con migliore validation_accuracy FATTO\n","#TODO usare solo un log, rendere in uscita metriche di interesse FATTO\n","\n","print(f'Accuracy report on TEST: {test_accuracy*100}%')\n","test_metrics = {\"Test Accuracy\": test_accuracy}\n","wandb.log({**test_metrics})\n","\n","wandb.finish()"],"metadata":{"id":"QVxkCSoNEIb2"},"id":"QVxkCSoNEIb2","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#track run, depth = 14\n","run = wandb.init(\n","    # Set the project where this run will be logged\n","    project=\"Lab1-Final\",\n","    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n","    name=f\"CNN depth 14\",\n","    # Track hyperparameters and run metadata\n","    config={\n","    \"initial_learning_rate\": 0.1,\n","    \"architecture\": \"CNN\",\n","    \"dataset\": \"Cifar10\",\n","    \"epochs\": 200,\n","    \"batch_size\": 128,\n","    \"validation_set_size\": 5000,\n","    \"layers\": [2, 2, 2]}) #TODO aggiungere depth FATTO\n","\n","#Split into train and validation\n","ds_train, ds_val = torch.utils.data.random_split(trainset, [50000 - run.config[\"validation_set_size\"], run.config[\"validation_set_size\"]])\n","\n","# Dataloaders\n","dl_train = torch.utils.data.DataLoader(ds_train, batch_size=run.config[\"batch_size\"], shuffle=True, num_workers=2)\n","dl_val   = torch.utils.data.DataLoader(ds_val, batch_size=run.config[\"batch_size\"], num_workers=2)\n","dl_test = torch.utils.data.DataLoader(ds_test, batch_size=run.config[\"batch_size\"], shuffle=False, num_workers=2)\n","\n","#Training and Test\n","device = 'cuda'\n","\n","modelCNN = CNN(layers=[1,1,1]).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(modelCNN.parameters(), lr=run.config[\"initial_learning_rate\"])\n","scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1, end_factor=0.2, total_iters=100)\n","\n","torch.manual_seed(111)\n","best_accuracy = 0\n","state_dict = None\n","for epoch in range(run.config[\"epochs\"]):\n","    (train_loss, train_corr) = train(modelCNN, optimizer, dl_train, criterion, epoch)\n","    train_accuracy = (train_corr / (50000 - run.config[\"validation_set_size\"])) * 100\n","    metrics = {\"train_loss\": train_loss,\n","               \"train_accuracy\": train_accuracy}\n","\n","    (validation_loss, validation_accuracy) = evaluate(modelCNN, dl_val, run.config[\"validation_set_size\"], criterion)\n","    if validation_accuracy >= best_accuracy:\n","        state_dict = modelCNN.state_dict()\n","    val_metrics = {\"validation_loss\": validation_loss,\n","                   \"validation_accuracy\": validation_accuracy}\n","\n","    scheduler.step()\n","\n","    wandb.log({**metrics, **val_metrics})\n","\n","#Test\n","modelCNN.load_state_dict(state_dict)\n","\n","#save model on weights and biases\n","model_artifact = wandb.Artifact(\n","    \"CNN\", type=\"model\",\n","    description=\"best model\",\n","    metadata=dict(run.config))\n","\n","torch.save(modelCNN.state_dict(), \"best_model_cnn.pth\")\n","model_artifact.add_file(\"best_model_cnn.pth\")\n","wandb.save(\"best_model_cnn.pth\")\n","run.log_artifact(model_artifact)\n","\n","(_, test_accuracy) = evaluate(modelCNN, dl_test, 10000, criterion, test=True) #TODO usare modello con migliore validation_accuracy FATTO\n","#TODO usare solo un log, rendere in uscita metriche di interesse FATTO\n","\n","print(f'Accuracy report on TEST: {test_accuracy*100}%')\n","test_metrics = {\"Test Accuracy\": test_accuracy}\n","wandb.log({**test_metrics})\n","\n","wandb.finish()"],"metadata":{"id":"IAfVhEIJEVEW"},"id":"IAfVhEIJEVEW","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"8e1dbcb0","metadata":{"id":"8e1dbcb0"},"source":["Runs with Residual connection"]},{"cell_type":"code","execution_count":null,"id":"FmX7IuFjz29m","metadata":{"id":"FmX7IuFjz29m"},"outputs":[],"source":["#track run, depth = 8\n","run = wandb.init(\n","    # Set the project where this run will be logged\n","    project=\"Lab1-Final\",\n","    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n","    name=f\"ResidualCNN depth 8\",\n","    # Track hyperparameters and run metadata\n","    config={\n","    \"initial_learning_rate\": 0.1,\n","    \"architecture\": \"ResidualCNN\",\n","    \"dataset\": \"Cifar10\",\n","    \"epochs\": 200,\n","    \"batch_size\": 128,\n","    \"validation_set_size\": 5000,\n","    \"layers\": [1, 1, 1]}) #TODO aggiungere depth FATTO\n","\n","# Dataloaders\n","dl_train = torch.utils.data.DataLoader(ds_train, batch_size=run.config[\"batch-size\"], shuffle=True, num_workers=2)\n","dl_val   = torch.utils.data.DataLoader(ds_val, batch_size=run.config[\"batch-size\"], num_workers=2)\n","dl_test = torch.utils.data.DataLoader(ds_test, batch_size=run.config[\"batch-size\"], shuffle=False, num_workers=2)\n","\n","#Training and Test\n","device = 'cuda'\n","\n","modelCNN = CNN(layers=run.config[\"layers\"], residual=True).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(modelCNN.parameters(), lr=run.config[\"initial_learning_rate\"])\n","scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1, end_factor=0.2, total_iters=100)\n","\n","torch.manual_seed(111)\n","best_accuracy = 0\n","state_dict = None\n","for epoch in range(run.config[\"epochs\"]):\n","    (train_loss, train_corr) = train(modelCNN, optimizer, dl_train, criterion, epoch)\n","    train_accuracy = (train_corr / (60000 - run.config[\"validation_set_size\"])) * 100\n","    metrics = {\"train_loss\": train_loss,\n","               \"train_accuracy\": train_accuracy}\n","\n","    (validation_loss, validation_accuracy) = evaluate(modelCNN, dl_val, run.config[\"validation_set_size\"], criterion)\n","    if validation_accuracy >= best_accuracy:\n","        state_dict = modelCNN.state_dict()\n","    val_metrics = {\"validation_loss\": validation_loss,\n","                   \"validation_accuracy\": validation_accuracy}\n","\n","    scheduler.step()\n","\n","    wandb.log({**metrics, **val_metrics})\n","\n","#Test\n","modelCNN.load_state_dict(state_dict)\n","\n","#save model on weights and biases\n","model_artifact = wandb.Artifact(\n","    \"ResidualCNN\", type=\"model\",\n","    ddescription=\"best model\",\n","    metadata=dict(run.config))\n","\n","torch.save(modelMLP.state_dict(), \"best_model_rcnn.pth\")\n","model_artifact.add_file(\"best_model_rcnn.pth\")\n","wandb.save(\"best_model_rcnn.pth\")\n","run.log_artifact(model_artifact)\n","\n","(_, test_accuracy) = evaluate(modelMLP, dl_test, 10000, criterion, test=True) #TODO usare modello con migliore validation_accuracy FATTO\n","#TODO usare solo un log, rendere in uscita metriche di interesse FATTO\n","\n","print(f'Accuracy report on TEST: {test_accuracy*100}%')\n","test_metrics = {\"Test Accuracy\": test_accuracy}\n","wandb.log({**test_metrics})\n","\n","wandb.finish()"]},{"cell_type":"code","source":["#track run, depth = 10\n","run = wandb.init(\n","    # Set the project where this run will be logged\n","    project=\"Lab1-Final\",\n","    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n","    name=f\"ResidualCNN depth 10\",\n","    # Track hyperparameters and run metadata\n","    config={\n","    \"initial_learning_rate\": 0.1,\n","    \"architecture\": \"ResidualCNN\",\n","    \"dataset\": \"Cifar10\",\n","    \"epochs\": 200,\n","    \"batch_size\": 128,\n","    \"validation_set_size\": 5000,\n","    \"layers\": [1, 2, 1]}) #TODO aggiungere depth FATTO\n","\n","# Dataloaders\n","dl_train = torch.utils.data.DataLoader(ds_train, batch_size=run.config[\"batch-size\"], shuffle=True, num_workers=2)\n","dl_val   = torch.utils.data.DataLoader(ds_val, batch_size=run.config[\"batch-size\"], num_workers=2)\n","dl_test = torch.utils.data.DataLoader(ds_test, batch_size=run.config[\"batch-size\"], shuffle=False, num_workers=2)\n","\n","#Training and Test\n","device = 'cuda'\n","\n","modelCNN = CNN(layers=run.config[\"layers\"], residual=True).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(modelCNN.parameters(), lr=run.config[\"initial_learning_rate\"])\n","scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1, end_factor=0.2, total_iters=100)\n","\n","torch.manual_seed(111)\n","best_accuracy = 0\n","state_dict = None\n","for epoch in range(run.config[\"epochs\"]):\n","    (train_loss, train_corr) = train(modelCNN, optimizer, dl_train, criterion, epoch)\n","    train_accuracy = (train_corr / (60000 - run.config[\"validation_set_size\"])) * 100\n","    metrics = {\"train_loss\": train_loss,\n","               \"train_accuracy\": train_accuracy}\n","\n","    (validation_loss, validation_accuracy) = evaluate(modelCNN, dl_val, run.config[\"validation_set_size\"], criterion)\n","    if validation_accuracy >= best_accuracy:\n","        state_dict = modelCNN.state_dict()\n","    val_metrics = {\"validation_loss\": validation_loss,\n","                   \"validation_accuracy\": validation_accuracy}\n","\n","    scheduler.step()\n","\n","    wandb.log({**metrics, **val_metrics})\n","\n","#Test\n","modelCNN.load_state_dict(state_dict)\n","\n","#save model on weights and biases\n","model_artifact = wandb.Artifact(\n","    \"ResidualCNN\", type=\"model\",\n","    ddescription=\"best model\",\n","    metadata=dict(run.config))\n","\n","torch.save(modelMLP.state_dict(), \"best_model_rcnn.pth\")\n","model_artifact.add_file(\"best_model_rcnn.pth\")\n","wandb.save(\"best_model_rcnn.pth\")\n","run.log_artifact(model_artifact)\n","\n","(_, test_accuracy) = evaluate(modelMLP, dl_test, 10000, criterion, test=True) #TODO usare modello con migliore validation_accuracy FATTO\n","#TODO usare solo un log, rendere in uscita metriche di interesse FATTO\n","\n","print(f'Accuracy report on TEST: {test_accuracy*100}%')\n","test_metrics = {\"Test Accuracy\": test_accuracy}\n","wandb.log({**test_metrics})\n","\n","wandb.finish()"],"metadata":{"id":"_mcJm5pdUhxM"},"id":"_mcJm5pdUhxM","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#track run, depth = 14\n","run = wandb.init(\n","    # Set the project where this run will be logged\n","    project=\"Lab1-Final\",\n","    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n","    name=f\"ResidualCNN depth 14\",\n","    # Track hyperparameters and run metadata\n","    config={\n","    \"initial_learning_rate\": 0.1,\n","    \"architecture\": \"ResidualCNN\",\n","    \"dataset\": \"Cifar10\",\n","    \"epochs\": 200,\n","    \"batch_size\": 128,\n","    \"validation_set_size\": 5000,\n","    \"layers\": [2, 2, 2]}) #TODO aggiungere depth FATTO\n","\n","# Dataloaders\n","dl_train = torch.utils.data.DataLoader(ds_train, batch_size=run.config[\"batch-size\"], shuffle=True, num_workers=2)\n","dl_val   = torch.utils.data.DataLoader(ds_val, batch_size=run.config[\"batch-size\"], num_workers=2)\n","dl_test = torch.utils.data.DataLoader(ds_test, batch_size=run.config[\"batch-size\"], shuffle=False, num_workers=2)\n","\n","#Training and Test\n","device = 'cuda'\n","\n","modelCNN = CNN(layers=run.config[\"layers\"], residual=True).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(modelCNN.parameters(), lr=run.config[\"initial_learning_rate\"])\n","scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1, end_factor=0.2, total_iters=100)\n","\n","torch.manual_seed(111)\n","best_accuracy = 0\n","state_dict = None\n","for epoch in range(run.config[\"epochs\"]):\n","    (train_loss, train_corr) = train(modelCNN, optimizer, dl_train, criterion, epoch)\n","    train_accuracy = (train_corr / (60000 - run.config[\"validation_set_size\"])) * 100\n","    metrics = {\"train_loss\": train_loss,\n","               \"train_accuracy\": train_accuracy}\n","\n","    (validation_loss, validation_accuracy) = evaluate(modelCNN, dl_val, run.config[\"validation_set_size\"], criterion)\n","    if validation_accuracy >= best_accuracy:\n","        state_dict = modelCNN.state_dict()\n","    val_metrics = {\"validation_loss\": validation_loss,\n","                   \"validation_accuracy\": validation_accuracy}\n","\n","    scheduler.step()\n","\n","    wandb.log({**metrics, **val_metrics})\n","\n","#Test\n","modelCNN.load_state_dict(state_dict)\n","\n","#save model on weights and biases\n","model_artifact = wandb.Artifact(\n","    \"ResidualCNN\", type=\"model\",\n","    ddescription=\"best model\",\n","    metadata=dict(run.config))\n","\n","torch.save(modelMLP.state_dict(), \"best_model_rcnn.pth\")\n","model_artifact.add_file(\"best_model_rcnn.pth\")\n","wandb.save(\"best_model_rcnn.pth\")\n","run.log_artifact(model_artifact)\n","\n","(_, test_accuracy) = evaluate(modelMLP, dl_test, 10000, criterion, test=True) #TODO usare modello con migliore validation_accuracy FATTO\n","#TODO usare solo un log, rendere in uscita metriche di interesse FATTO\n","\n","print(f'Accuracy report on TEST: {test_accuracy*100}%')\n","test_metrics = {\"Test Accuracy\": test_accuracy}\n","wandb.log({**test_metrics})\n","\n","wandb.finish()"],"metadata":{"id":"deY2C_r_Ujyj"},"id":"deY2C_r_Ujyj","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"ef4de2f2-abc5-4f98-9eaf-3497f734a022","metadata":{"id":"ef4de2f2-abc5-4f98-9eaf-3497f734a022"},"source":["-----\n","## Exercise 2: Choose at Least One\n","\n","Below are **three** exercises that ask you to deepen your understanding of Deep Networks for visual recognition. You must choose **at least one** of the below for your final submission -- feel free to do **more**, but at least **ONE** you must submit."]},{"cell_type":"markdown","id":"07978e8e-9f2e-4949-9699-495af6cb6349","metadata":{"id":"07978e8e-9f2e-4949-9699-495af6cb6349"},"source":["### Exercise 2.1: Explain why Residual Connections are so effective\n","Use your two models (with and without residual connections) you developed above to study and **quantify** why the residual versions of the networks learn more effectively.\n","\n","**Hint**: A good starting point might be looking at the gradient magnitudes passing through the networks during backpropagation."]},{"cell_type":"code","execution_count":null,"id":"469e81a3-08ca-4549-a2f8-f47cf5a0308b","metadata":{"id":"469e81a3-08ca-4549-a2f8-f47cf5a0308b","tags":[]},"outputs":[],"source":["# Your code here."]},{"cell_type":"markdown","id":"440a3a7b-2ed6-4f58-a1b7-5ab1fc432893","metadata":{"id":"440a3a7b-2ed6-4f58-a1b7-5ab1fc432893"},"source":["### Exercise 2.2: Fully-convolutionalize a network.\n","Take one of your trained classifiers and **fully-convolutionalize** it. That is, turn it into a network that can predict classification outputs at *all* pixels in an input image. Can you turn this into a **detector** of handwritten digits? Give it a try.\n","\n","**Hint 1**: Sometimes the process of fully-convolutionalization is called \"network surgery\".\n","\n","**Hint 2**: To test your fully-convolutionalized networks you might want to write some functions to take random MNIST samples and embed them into a larger image (i.e. in a regular grid or at random positions)."]},{"cell_type":"code","execution_count":null,"id":"9e33c912-0716-44ef-a91b-47ca19a2b2cd","metadata":{"id":"9e33c912-0716-44ef-a91b-47ca19a2b2cd","tags":[]},"outputs":[],"source":["# Your code here."]},{"cell_type":"markdown","id":"8243f811-8227-4c6f-b07f-56e8cd91643a","metadata":{"id":"8243f811-8227-4c6f-b07f-56e8cd91643a"},"source":["### Exercise 2.3: *Explain* the predictions of a CNN\n","\n","Use the CNN model you trained in Exercise 1.2 and implement [*Class Activation Maps*](http://cnnlocalization.csail.mit.edu/#:~:text=A%20class%20activation%20map%20for,decision%20made%20by%20the%20CNN.):\n","\n","> B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning Deep Features for Discriminative Localization. CVPR'16 (arXiv:1512.04150, 2015).\n","\n","Use your implementation to demonstrate how your trained CNN *attends* to specific image features to recognize *specific* classes.\n","\n","**Note**: Feel free to implement [Grad-CAM](https://arxiv.org/abs/1610.02391) instead of CAM."]},{"cell_type":"code","execution_count":null,"id":"d634a700-56c2-48fd-96e0-4c94d1bd0cfe","metadata":{"id":"d634a700-56c2-48fd-96e0-4c94d1bd0cfe","tags":[]},"outputs":[],"source":["# Load model\n","run = wandb.init()\n","\n","artifact = run.use_artifact(\"best_model_rcnn.pth\", type='model')\n","artifact_dir = artifact.download()\n","\n","wandb.finish()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"},"widgets":{"application/vnd.jupyter.widget-state+json":{"619c0362f6694bc1b6840091bc57cf13":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_0c05f5e6b87d434b8bb5732578a0e179","IPY_MODEL_9185de64b3174405883a4df197ec1ef5"],"layout":"IPY_MODEL_b9cee0a6a44d486b8411b74c9442f3ef"}},"0c05f5e6b87d434b8bb5732578a0e179":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a2be62525cc43e8b325f9c73a3ff7cd","placeholder":"​","style":"IPY_MODEL_7b556393eaae4da6a8b5bcc0ac7a44cf","value":"0.019 MB of 0.019 MB uploaded\r"}},"9185de64b3174405883a4df197ec1ef5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_317acbbca7f94f0b9d46c9b24e7f9cb7","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b2c32e1c27174a2fa62d6967052528bc","value":1}},"b9cee0a6a44d486b8411b74c9442f3ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a2be62525cc43e8b325f9c73a3ff7cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b556393eaae4da6a8b5bcc0ac7a44cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"317acbbca7f94f0b9d46c9b24e7f9cb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2c32e1c27174a2fa62d6967052528bc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"44b0a6677852405bb53933cc9e3f633e":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_c7d57407669c420884ad0d712442626e","IPY_MODEL_1c458cbed5d84b42a3700d479f485335"],"layout":"IPY_MODEL_32b64d4a92c74fc6a1397e9b859795a1"}},"c7d57407669c420884ad0d712442626e":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a9f735bc7c6470c9b2c7e6a82a2eea6","placeholder":"​","style":"IPY_MODEL_0f9c6c3e1eaf4111a79eb20df2fab9c3","value":"0.018 MB of 0.018 MB uploaded\r"}},"1c458cbed5d84b42a3700d479f485335":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_d680315b0a7944f9845104aa59bc1a9f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_32e4d2ed44fb4f1fa10e52e1019cd2e6","value":1}},"32b64d4a92c74fc6a1397e9b859795a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a9f735bc7c6470c9b2c7e6a82a2eea6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f9c6c3e1eaf4111a79eb20df2fab9c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d680315b0a7944f9845104aa59bc1a9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32e4d2ed44fb4f1fa10e52e1019cd2e6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"03a4b7ca41e249f2926f497ebdbefd56":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_329190c8f792403aab0ba443bf2c06ad","IPY_MODEL_01ba87de5f1a4f8a9cf85598ce559f4e"],"layout":"IPY_MODEL_88966ea1556349abbf8170c4f57d0b33"}},"329190c8f792403aab0ba443bf2c06ad":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49f856f725b649a38b098407d4c865cd","placeholder":"​","style":"IPY_MODEL_f3bcdfb4422244489eabb1a6e8f3c6f4","value":"0.018 MB of 0.018 MB uploaded\r"}},"01ba87de5f1a4f8a9cf85598ce559f4e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a3c3fdf4519488ab27e34e77b571b6b","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e251ddeffd1b48d88bf4ffea4272bd03","value":1}},"88966ea1556349abbf8170c4f57d0b33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49f856f725b649a38b098407d4c865cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3bcdfb4422244489eabb1a6e8f3c6f4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9a3c3fdf4519488ab27e34e77b571b6b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e251ddeffd1b48d88bf4ffea4272bd03":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":5}